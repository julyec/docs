const e=JSON.parse('{"key":"v-024c1db2","path":"/imports/chunk-reading.html","title":"Chunk reading","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"Using it together with Batch Inserts","slug":"using-it-together-with-batch-inserts","link":"#using-it-together-with-batch-inserts","children":[]},{"level":2,"title":"Keep Track of the Row number","slug":"keep-track-of-the-row-number","link":"#keep-track-of-the-row-number","children":[]}],"git":{"createdTime":1704429178000,"updatedTime":1704429178000,"contributors":[{"name":"Moments","email":"554676262@qq.com","commits":1}]},"readingTime":{"minutes":1.13,"words":340},"filePathRelative":"imports/chunk-reading.md","localizedDate":"January 5, 2024","excerpt":"<h1> Chunk reading</h1>\\n\\n<p>Importing a large file can have a huge impact on the memory usage, as the library will try to load the entire sheet into memory.</p>\\n<p>To mitigate this increase in memory usage, you can use the <code>WithChunkReading</code> concern. This will read the spreadsheet in chunks and keep the memory usage under control.</p>"}');export{e as data};
